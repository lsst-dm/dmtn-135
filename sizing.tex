\section{Sizing inputs}

The following simplified sizing was used to give the input sizes for the cost model in \secref{sec:cost}.
The storage sizes are given in \tabref{tab:storageFloor} and \tabref{tab:chileStorage} while the compute is given in \tabref{tab:drpAndAlertSizing} and \tabref{tab:lspSizing}.


\subsection{Storage Model}

\input{storageSizing}

\subsubsection{Overview}

This simplified storage model eliminates many details in the previous storage model \citedsp{LDM-141} that end up being insignificant.
There are relatively few data products that require significant amounts of fast SSD or slower disk or tape storage; the others complicate the model without giving much insight.
In addition, it is assumed that bandwidth is not a significant constraint, other than the distinction between SSD and spinning disk.
With the advent of highly-parallel shared and object storage, having large numbers of spindles solely to achieve high bandwidth for certain operations is not thought to be necessary.

Values are computed for the amount of storage expected to be "on the floor" at the beginning of each fiscal year from FY2020 through FY2023 (which is LSST Operations Year 1).
Not included is any storage already present at the end of FY2019 holding past data.

Key scientific and algorithmic assumptions made include:
\begin{itemize}
\item All significant intermediates and data products generated by Data Release Production processing need to be kept on filesystem disk until the DRP is complete.
Some scratch space is provided to hold small, temporary intermediates.
If some intermediates could be removed during DRP when it is known they will no longer be needed, some space savings could be realized.
\item HSC RC2 processing is representative of the outputs that DRP will generate.
In particular, the number of coadds and the presence or absence of "heavy footprints" are assumed to be correct.
\item Raw science images, processed visit images (PVIs), coadds, and catalogs in Parquet format start on "normal" filesystem disk but then move to object storage at the completion of the DRP, with lossy compression of the PVIs at that time.
This is in accordance with \jira{RFC-325}, although the relevant LCR has not yet been approved.
\item Intermediates like warped images for coaddition are not survey data products and do not need to be kept beyond the end of the DRP and subsequent QA.
\end{itemize}

All data is backed up to tape permanently, including annual snapshots of filesystems.
Any incremental backups are assumed to be reusable or otherwise purged and hence not significant.

\subsubsection{Parameters}

The key parameters in \tabref{tab:storageSizing} are described below.

The numbers of Objects, Sources, and ForcedSources are taken from \citeds{LSE-81}, with the FY2022 numbers reduced by a factor of 2/12 to account for the anticipated 2~months of on-sky science validation time for LSSTCam before the survey begins.
These numbers are ultimately based on models for stars in the galaxy and galaxies in the universe that are dependent on the limiting magnitude achieved in each year.

The numbers of science users are estimates, using "Stack Club" users and Commissioning users for FY2020 and 2021, followed by US science users in FY2022 and FY2023 for Data Preview data.
The bulk of US science users are not expected to arrive until after Data Release~1 at the beginning of FY2024.

Storage per science user is estimated based on today's usage at NCSA, scaled up as users become more active, and approaching the number given in \citeds{LSE-81} as Operations begins.
Note that it is expected that there will be a wide distribution of usage by user, with some using almost none and some using much more than their proportional share.

The LSSTCam image size is uncompressed and includes overscan, 4~bytes of raw data per pixel, and both science and corner rafts.

The raw image compression factor was measured on simulated LSST images.
The lossy image compression factor for processed visit images is the ratio between the lossy-compressed file size (estimated at 1/6 of uncompressed) and the lossless-compressed file size (estimated at 66\% of uncompressed).
Note that PVIs do not compress losslessly as well as raw images due to their floating point planes.

The number of observing nights per year and the number of visits per night are maximal estimates.
2~images per visit is still the baseline and a possibility that must be accounted for.
The number of calibration images per day was derived from the calibration plan.

As stated above, the number of LSSTCam science images is scaled by 2/12 for FY2022 given the length of science validation time.
The number of test images is estimated as a ramp up to the full science cadence.
The numbers of engineering and calibration images are estimated as ramping-down fractions of the number of science and test images, with calibration images ending at the number per day given previously.

Sizes of rows in various data product tables are taken from \citeds{LDM-141}, which was in turn derived from the DPDD.

Qserv replicates its data for fault tolerance; a typical replication factor is selected here.

\subsubsection{Data Product Sizing}

Images and the results of processing them are the dominant factor controlling tthe storage sizing which is outlined in \tabref{tab:datasetSizing}.
Precursor survey and LSSTCam images are the largest; ComCam, at less than 5\% of the size of LSSTCam and with little on-sky science time is negligible, as is LATISS, which is less than 1\% of the size of LSSTCam, though it has considerable on-sky time.

The sizing of the Alert Production Database (APDB) is based on experiments in \cite{DMTN-113} which found that 57,000 visits took 4.5~TB including indexes.
A simple linear scaling to a full year's visits was performed, with half that purchased in 2020 for large (but not full) scale testing.

HyperSuprime-Cam (HSC) RC2 is a relatively small dataset used for monthly processing tests, but it is highly representative of the currently-known DRP work and so is used as the basis for scaling.
The size of the input images was taken from \cite{DMTN-091}; the size of the outputs (image and Parquet/other non-image files) was measured from the latest execution.
A similar size dataset based on DESC DC2 is assumed to be being used for an additional monthly processing test.
Note that this is a very small subset of the full DESC DC2, which is expected to cover 300~square degrees to 10-year LSST depth (approximately 1000 epochs per point on the sky).
The full DESC DC2 is not currently scheduled to be reprocessed by the construction team.
Instead, twice-a-year processings of the full HSC SSP PDR2 dataset (including PDR1) are assumed to occur.
The size of this dataset was measured on disk; it is 2,564,358 CCD images, each at 18.2~MB (approximately three times the size of PDR1 alone).

Output sizes are assumed to scale linearly with input size, and by the same factor for each instrument, except for coadds which scale by the sky area processed.
While the Object catalog ought to be proportional to sky area as well, its size is expected to be dominated by Source and ForcedSource, so we conservatively make them all proportional to input size (visits).

Scratch space is set at 10\% of the output image storage for LSSTCam processing; it is assumed to be already present for precursor processing.

Qserv Czar fast (SSD) storage is assumed to be used for the primary Object table; additional space for the so-called "secondary index" mapping object identifiers to spatial chunks is negligible in comparison.

The main Qserv database storage is based on the Parquet file sizing for precursor data and on the estimated numbers of Objects, Sources, and ForcedSources for LSSTCam data.

Note that no space is explicitly reserved for Qserv query result storage.

An additional 20\% disk and tape storage is added to account for all other needs.

\input{datasetSizing}

\subsubsection{Storage Sizing}

Finally, storage is allocated to specific types as shown in \tabref{tab:storageFloor}.
Fast storage (SSD) is used for the APDB and Qserv Czar, which accumulates data from year to year until Data Releases are retired.
Normal storage is used for inputs, scratch, and output images (initially).
Local Qserv storage is used for Qserv catalogs.
It is assumed that precursor data will be removed from Qserv once LSST data is available, but the LSST data accumulates from year to year.
Object storage is used for output tables each year and output images after one year.
Lossy compression is applied at this time.
Since only one year of operational processing is in the model, nothing is removed from the object store; it accumulates from year to year.
Tape is used for long-term archiving and filesystem backup.
Again, this accumulates from year to year.

Note that no replication is assumed in the object store.

\input{storageFloor}

An additional table (\tabref{tab:chileStorage}) gives the storage needs in the Chilean Data Access Center (DAC).
This comprises Qserv fast and local storage plus the data products in object storage.
Since no DRP computation occurs in Chile, no "normal" filesystem disk is required.
Chilean user home directories are assumed to be negligible at this level.

\input{chileStorage}

\subsection{Compute Model}

\input{computeSizing}

\subsubsection{Overview}
This simplified computing model (\tabref{tab:computeSizing}) divides computation into three classes: Data Release Production (DRP), Alert Production, and LSST Science Platform (for the US DAC, Chilean DAC, and LSST staff internal use).
Calibration Products Production is assumed to be negligible.

The pipelines have advanced considerably in terms of fidelity and science performance since the previous computing model \citedsp{LDM-138} was developed.
Scaling compute needs based on an execution of the nascent DRP pipeline on HSC PDR1 data and nightly executions of the nascent \texttt{ap\_pipe} pipeline on HiTS2015 data is thus appropriate, but the fact that several steps are still missing from these pipelines must be taken into account.

Elapsed times are measured on existing hardware and converted into core-hours on a nominal CPU (Intel Xeon E5-2680v3 at 2.50~GHz).
This estimation methodology incorporates all I/O, memory bandwidth, cache miss, and other overheads into the core-hour measurement, simplifying calculations.
Note that the nominal CPU does not evolve with time; if future CPUs do more work per core, the actual core-hours may be less than estimated here.

Key scientific and algorithmic assumptions are:
\begin{itemize}
\item DRP compute time is proportional to the input data size (or, equivalently, the number of visits).
While certain tasks are undoubtedly proportional to sky area or number of Objects, overall the pipeline elapsed times are a better fit to the number of visits.
Some of this may be because the Object density increases as the number of visits to the same sky patch increases.
\item HSC PDR1 processing is generally representative of the final DRP, with an allocation for future additional steps as described below.
\item Qserv core counts should remain proportional to the size of data loaded into the database in order to maintain sufficient disk bandwidth and query processing capability.
\item The US DAC LSP is sized at 10\% of the DRP compute budget.
The Chilean DAC LSP is sized at 20\% of the US DAC (as in \citeds{LDM-138}).
The LSST staff LSP is sized at 10\% of the US DAC.
\end{itemize}

\subsubsection{Parameters}
The key parameters in \tabref{tab:computeSizing} are described below.

HSC PDR1 was executed on the NCSA verification cluster, which uses the nominal CPU.
The Alert Production executes on Kubernetes nodes, which are a bit slower; to be conservative, this is neglected.

The most recent run of DRP on HSC PDR1 data is described at \url{https://confluence.lsstcorp.org/x/WpBiB}.
The input data size is measured.
Most jobs (but not most of the time) could run on relatively small-memory machines with 24~cores and 5~GB RAM per core.
The largest and longest-running jobs, however, required up to 4~times as much memory, using half or a quarter of the cores.
To be conservative, we assume that half the cores were used for the large-memory jobs.
The percentage of DRP core-hours that will need to execute on large-memory nodes is estimated.

Since the HSC PDR1 processing did not include several steps from the Science Pipelines Design document \citedsp{LDM-151} such as image differencing and full multi-epoch characterization, the core-hours used are scaled up to the expected pipeline consumption.
Note that these algorithmic adjustments are multiplicative.

The SQuaSH system reports the execution time of \texttt{ap\_pipe} in seconds per CCD.
A mean was taken over all processed CCDs, and it was assumed that each CCD is processed on a single core.
A factor is added to account for additional steps like differential chromatic refraction compensation and false positive detection that are not well-represented in the current pipeline.
Multiplying by the number of LSSTCam science CCDs gives the total number of core-hours per visit.

\subsubsection{Data Release Production}

The number of nominal core-hours per TB of input data is multiplied by the precursor (HSC RC2 and DESC DC2 subset for 12~months and HSC PDR2 twice a year) and LSSTCam input data sizes to determine the total number of core-hours needed in each year.
This is shown in \tabref{tab:drpAndAlertSizing}.
Approximately one-third of these core-hours need to be provided by small-memory (4-5~GB/core) machines; the other two-thirds need to come from large-memory (8-20~GB/core) machines.

\input{drpAndAlertSizing}

\subsubsection{Alert Production}

The core-hours per visit are divided by the minimum visit length (30~sec plus 1~sec shutter motion plus 2~sec readout) to give the minimum number of cores needed to keep up with image taking.
This is shown in \tabref{tab:drpAndAlertSizing}.
These cores are expected to be provided over multiple "strings" of nodes.
Note that the current AP design is not readily able to take advantage of more than one core per CCD.

\subsubsection{LSST Science Platform}

LSST Science Platform needs for US DAC science users are derived as 10\% of the DRP core-hour requirement and are shown in \tabref{tab:lspSizing}.
The LSP core-hours are assumed to be spread over a year, giving the total number of nominal cores needed in the DAC.
Peak loads are expected to be handled by "borrowing" elastically from the DRP compute pool.

As a reasonableness check, the number of cores per science user is computed, but it must be noted that an oversubscription factor needs to be taken into account since not all users are expected to be simultaneously active.

Similar computations for the Chilean DAC (at 20\% of the US DAC) and the LSST staff LSP (at 10\% of the US DAC) are also in \tabref{tab:lspSizing}.

Note that staff use of Qserv is taken into account by loading the Data Release products into an internal-only Qserv instance and then making that instance part of the DAC at Data Release, so the compute sizing is part of the US DAC.

\input{lspSizing}

\subsubsection{DES Comparison}

As another check on the model, core-hour figures for Dark Energy Survey (DES) processing were obtained.
These are given in \tabref{tab:desComparison}.
The CPUs used for single-frame and coadd processing had slightly slower clock rates but better bandwidths and expected instructions per clock performance, so they were considered equivalent to our nominal core.
The CPUs used for Multi-Object Multi-Band Fitting and Single-Object Fitting (MOF/SOF) included a large contribution from the Blue Waters machine at NCSA.
Those CPUs (AMD~6276) are somewhat older and were estimated at 0.245~nominal cores.

The single-frame processing measured number of 5.2~core-hours per visit compares well with the 5.4 core-hour per visit parameter used in our sizing model.
Similarly, the overall DES compute figure of 21,000 core-hours per terabyte is virtually identical to our estimate (including the factors for additional steps).

\input{desComparison}


\subsection{Operations Sizing}

Five tables use some of the parameters from the above model to project LSST storage and compute needs throughout the 10~years of Operations.

\subsubsection{Storage in Operations}

\afterpage{
  \clearpage
  \begin{landscape}
      \input{storageSizingOps}
      \input{datasetSizingOps}
  \end{landscape}
  \clearpage
}

The Object, Source, and ForcedSource numbers in \tabref{tab:storageSizingOps} are taken from \citeds{LSE-81}, as before.
The number of science users and storage per user is ramped up.
Note that the number of images needing storage and processing grows linearly with time.
Table row sizes are taken from \citeds{LDM-141}; they include growth over time as columns are added.

The dataset sizes in \tabref{tab:datasetSizingOps} are calculated using the same formulas and proportionality constants as in \tabref{tab:datasetSizing}.

\afterpage{
  \clearpage
  \begin{landscape}
      \input{storageFloorOps}
      \input{chileStorageOps}
      \input{computeSizingOps}
  \end{landscape}
  \clearpage
}

The on-the-floor storage estimates in \tabref{tab:storageFloorOps} include fast (SSD) storage for the APDB and Qserv Czar, with the latter being sized for three Data Releases (two being served and one being prepared).

"Normal" filesystem storage holds raw images, data products, scratch space, Qserv data prior to loading, science user workspace, and a 20\% allocation for everything else.

Qserv local storage holds catalogs for three Data Releases.

Raw images (lossless-compressed) are copied to object storage.
Lossy-compressed PVIs, and catalogs in Parquet format are also moved there, with sizing for three Data Releases.

All data products and new raw images for each Data Release are copied to tape.

\tabref{tab:chileStorageOps} extracts the Qserv and object store sizing needed to populate the Chilean DAC with a copy of the data products and raw images.

\subsubsection{Compute in Operations}

The DRP compute sizing in \tabref{tab:computeSizingOps} follows directly from the size of the input data to be processed.
The number of cores for Alert Production does not change with time.
The DAC and staff LSP instances are sized based on the assumed percentages of DRP compute, with Qserv sized based on its catalog data size.
